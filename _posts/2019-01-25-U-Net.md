---
layout: "post"
title: "[Deep learning]U-Net: Biomedical Image Segmentation"
author: "HJ-harry"
mathjax: true
---

## Semantic Segmentation  


Convolutional Neural Network를 처음 배울 때 접하게 되며, 가장 간단한 task는 Image classification입니다. Image classification에서는 한 이미지에 해당하는 하나의 label을 output해주면 됩니다. 예를 들어 MNIST dataset의 경우 숫자가 0~9까지 10가지가 있고, 한 숫자 이미지를 넣었을 때 그 숫자가 어떤 숫자인지 0~9까지 하나의 label을 output해주면 됩니다. Loss function, 또는 objective function도 이 class prediction이 얼마나 정확한지에 따라 정의됩니다.  

하지만 이런 classification으로 풀 수 있는 task는 상당히 제한적입니다. Output을 하나밖에 줄 수 없기 때문입니다. 하지만 CNN으로 이런 일 이외에 오늘 설명할 semantic segmentation, object detection등 다양한 일들을 할 수 있고, 오늘 설명할 semantic segmentation은 classification 다음으로 기본이 되는 내용입니다.  

예컨대 어떤 사람의 CT scan이 있고, 그 CT scan에서 nodule을 찾고자 한다고 가정합시다. 우리는 nodule에 해당하는 부분은 1로, nodule이 아닌 그 이외의 부분은 0으로 표현하고자 합니다. 이미지는 pixel단위로 표현되기 때문에, 이 경우 이것을 **pixel-wise binary classification**으로 볼 수 있겠죠. 물론 이런 binary classification 이 아니라 multi-class classification도 가능하지만, biomedical classification에서는 이런 binary classification을 하게 되는 경우가 많습니다.  



![CNN Architecture](https://cdn-images-1.medium.com/max/2000/1*vkQ0hXDaQv57sALXAJquxA.jpeg)  
**(그림1)**


기존 Image classification task에서는 convolutional layer들이 다층으로 겹쳐져 있고, 마지막에는 Fully connected layer를 붙여서 결국 output label의 수만큼 각 class label에 대한 probability를 보여주거나, one-hot vector를 보여주는 방식을 취했습니다. 다시 말해서 **convolutional layer에서 학습한 Image의 representation들을 하나로 통합**해서 이 image가 어떤 image인지 판단하는 방식입니다.  

![U-net Architecture](http://blog.qure.ai/assets/images/segmentation-review/unet.png)
**(그림 2)**

그렇다면 pixel-wise classification은 어떻게 하는걸까요? Intuition은 간단합니다. **Fully connected layer가 있을자리를, Upsampling을 통해 다시 input 크기의 output을 만들어줍니다.** 이는 **Autoencoder**와 비슷한 구조라고 생각할 수도 있습니다. 단, Autoencoder는 output에서 input과 동일한 이미지를 만드는 것이 objective이고, semantic segmentation은 output에서 input의 pixel별로 classification을 하는 것이 objective입니다. 따라서 **objective function에서는 차이가 나지만, Encoding-Decoding을 한다는 점에서 비슷한 방식을 따릅니다.**



## Sliding Window 방식과의 차이

**Fully Convolutional Network**를 사용하기 이전에는 **Sliding window approach**를 사용하는 모델들이 mainstream이었습니다. [Ciresan et al.][http://people.idsia.ch/~juergen/nips2012.pdf]은 2012년에 발표된 논문으로 한 image를 **여러개의 patch**로 나누어 각 patch를 CNN 구조를 통해 학습시키게 됩니다. 이 방식은 몇 가지 단점이 있습니다.  

1. 나누어진 patch별로 network가 따로 학습을 진행해야 하기 때문에 속도가 매우 느립니다.
2. patch별로 overlap이 있다면 겹치는 부분은 같은 computation을 여러 번 하는 셈이므로 불필요한 연산을 하게 됩니다.
3. patch의 size에 따라 localization accuracy와 context learning 사이의 trade-off가 있습니다.
    1) patch가 크다면: localization accuracy가 떨어지게 됩니다.
    2) patch가 작다면: context를 잘 학습하지 못할 수 있습니다.

따라서 U-net 과 같은 Fully Convolutional Network에서는 patch를 나누는 방식을 사용하지 않고 image 하나를 그대로 네트워크에 집어넣으며, **context와 localization accuracy를 둘 다 취할 수 있는 방식을 제시합니다**.



## Network Architecture



(그림 2)가 U-net의 구조입니다. 네트워크의 이름인 U-net에서 알 수 있다시피, 네트워크가 U 모양을 따르고 있습니다. 우선, Encoding을 하는 부분은 일반적인 CNN의 구조를 따릅니다. **고차원의 image data를 저차원의 feature로 압축을 한다고 볼 수 있습니다.** 다만, Fully connected layer가 아닌 convolutional layer만 사용해서 downsampling을 하기 때문에 **spatial structure**는 유지할 수 있게 됩니다. 세부적인 spatial structure는 blurry해지겠지만, 대략적인 구조는 끌고 갈 수 있습니다. 나아가, 이런식으로 downsampling을 하게 되면 그렇지 않았을 때보다 computation의 양을 현저히 줄일 수 있습니다.    

Decoding을 하는 부분에서는 주목해야 할 부분이 두 가지 있습니다.  
1. Downsampling을 하여 줄어든 data의 크기를 [Upsampling][https://hj-harry.github.io/HJ-blog/2019/01/23/Transposed-Convolution.html]을 통해 다시 늘리는 구조를 취하고 있습니다.
2. Encoding을 한 부분에서 Upsampling만 하는 것이 아니라, Spatial context를 더 자세히 나타내는 **contraction 부분을 copy and crop해서 upsampling 부분에 concatenate합니다.** 이 때 channel의 수를 맞춰주기 위해 feature channel의 수는 반으로 줄이죠.

(그림 2)를 보면 알 수 있듯이 해당 논문에서는 convolution을 할 때 zero padding을 하지 않아서 convolution을 할 때마다 pixel의 수가 줄어드는 것을 확인할 수 있는데요, 이 때문에 contracting path보다 upsampling path에서 image patch의 크기가 더 작아져 있는 것을 확인할 수 있습니다. 따라서 concatenation을 할 때 그대로 concatenate하지 못하고 **crop** 해서 붙여야 하는데, 처음부터 padding을 하면서 convolution을 한다면 이런 작업을 거치지 않아도 size가 맞기 때문에 제가 구현할 때는 *padding = 'same'* 으로 두고 네트워크를 짰습니다. 요새는 zero padding이 아닌 **partial convolutional padding**과 같은 더 성능이 좋은 padding 기술이 나왔기 때문에, 오히려 padding을 이용하는 방식이 border부분의 accuracy를 더 높일 수 있지 않을까 생각합니다.  

Upsampling을 하는 방식에 대해서는 [Transposed Convolution][https://hj-harry.github.io/HJ-blog/2019/01/23/Transposed-Convolution.html] 포스트에 자세히 정리를 해 두었으니 참고하시면 좋을 것 같습니다 :)  

U-net에서는 weight initialization을 [He initialization][https://arxiv.org/abs/1502.01852]로 하였으며, activation function은 마지막 layer를 제외하고 **ReLU**를 이용했습니다.  

**Biomedical data의 특징으로 data의 양이 많지 않다**는 점을 들 수 있는데요, 이를 해결하기 위해서 여기서는 **data augmentation**을 꽤나 heavy하게 이용했습니다. 특히 biomedical data는 **elastic deformation**이 가장 흔하게 일어나서 elastic deformation이 적용된 image를 추가로 augment해서 training에 이용했습니다.  

### Reference
**[1]** [U-Net: Convolutional Networks for Biomedical Image Segmentation, Ronneberger et al. , 2015][https://arxiv.org/pdf/1505.04597.pdf]
**[2]** [Ciresan et al.][http://people.idsia.ch/~juergen/nips2012.pdf]
